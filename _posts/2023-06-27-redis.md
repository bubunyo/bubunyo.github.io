---
layout: post
title: "Redis"
description: "Why is redis fast, what are the pros and cons of using redis"
comments: true
keywords: "redis, infrastructure"
published: false
---

[Redis](https://redis.io) wass designed to be a blazingly fast key-value store to make caching easy. And for most parts it lives up to the billing. Setting up and using redis has always been a breeze. The success and popularity of redis has makes it it a very powerful tool in the software engineers arsenal, with its support of almost all data type and associated operations out of the box, it is a perfect solution for all persistence problems; almost. As with everything, people have used it in ways without consideration for if its fitness for the problem at hand often to great success.

Redis is famed for being able to perform operations that is measured in sub milliseconds. And the key to this is how it is designed. 
It is designed to store data in memory.  If your first year computer science class is anything to go by, you will know this provides much fast read and write throughput  compared to disk by miles. When we create data structures using variables we often use arrays, hashes and maps. Redis supports these and more. it is also effecient and manipulating them because, It takes advantage of the underlying data structures that are optimized for in memory storage without worrying about how to persist it to durable storage. It is also single threaded, Though a performant single thread system might sound counter intuitive, that are some peculiar performant advantages to it. And redis takes advantage of this in a brilliant way to ensure consistency without any cost to performance. Redis's single-thread will scales indefinitely in terms of I/O concurrency. It does this by using an I/O demultiplexing mechanism and a concise event loop designed by the author. Thus there is no synchronization to be done since all commands are serialized. It might look like CPU might become a bottle neck with this design but it turns out more often than not, you will hit a network bottleneck well before CPU cannot keep up. The positive side effect of this design is that atomicity of all operations come at no extra cost. Redis also use a [proprietary protocol](https://redis.io/docs/reference/protocol-spec/), that is much more terse.  Couple the isolated event loop with a proprietary communication protocol and you have a blazing fast in memory data store that scales indefinitely in theory and you have a highly preformant database. 
 
These facts only holds when the size of your payload and the number of connections remain relatively small. this easily jumps out the window with ever increasing load parameters. The threshold is unfortunately rather low at a high number connections and increased payload sizes. Modern large scale micro-services will easily have over 100 running instance at medium scale. And since most instances employing some pooling mechanism so as not to pay a connection cost for each request, A single redis instance is going to do a bit of work in maintaining those connections not to talk of serving requests as they come through. To improve performance at medium to high loads, some projects such as [KeyDb](https://docs.keydb.dev), Snapchats drop-in replacement as an alternative to redis, employs multithreaded approach and a bit of magic to sustain some high workloads. This has been touted to provide 5x performance over redis. Another solution which I have seen used is to employ a proxy that multiplexes over multiple redises. One such proxy is the twoemproxy developed at twitter. [Twoemproxy](https://github.com/twitter/twemproxy), or nutcracker as it is informally know, is in itself single threaded and employs key hashing to store keys in shards of multiple redis instance give you proper multiplexing. While it may look like this is susceptible to the original problems of a single threaded application, it is not necessarily the reality, because twoemproxy employs a single thread for each redis instance, turning the whole system into a multi threaded systems. Of course this is still susceptible to hotkeys. These might look like ideal solutions, setting up new infrastructure as an intermediary service introduces a new failure point and this is neither trivial nor ideal. But when done, there is a lot of net positives. 

Another major concern with redis is durability. Redis out of the box does not persist data on disk, only in memory. This means, when a server goes down, so does all you data. Durability is serious business and when it becomes a priority. And this is where redis starts to go backwards. Redis was never planned to be provide durable beyond RAM. This is evident in the fact that disk persistence, was never part of redis until, v 0.04. Redis supports two types of [persistence modes](https://redis.io/docs/management/persistence/).

The first one is called snapshotting which Redis calls RDB. When snapshotting is enabled redis will periodically write all your dataset in memory to disk. This is good for point in time recovery. But this also means, you loose all the data between when the last snapshot was created versus when the failure occurred. For a moderately busy server there are bound to be significant changes that happen between when snapshots are set, and loosing it might not be a good thing. To combat this, some teams, set the snapshots times as minimal as possible to combat the amount of data it is possible to loose. This can be a bad idea, when your data set is considerably large. Writing a 1gig file to disk every every 60 seconds, is a recipe for disaster. [What happens in the background](https://redis.io/docs/getting-started/faq/#background-saving-fails-with-a-fork-error-on-linux) is redis forks a child process for background processing, serialize in memory and make it disk compatible, write it to a temporary file in the background and rename the file atomically to  one upon finish. Even though the overhead of creating a fork is zero in theory when the OS supports copy-on-write, you still need to turn on the `overcommit_memory`. This is because if the dataset between the parent process and the child process deviate, redis will not keep track of the changes and will have to allocate just as much memory your data set has to the child process in order to snapshot successful. In snapshotting mode, you must do everything to ensure that, your datastore does not exceed half the RAM allocated to redis, otherwise your redis server will implode with an OOM. This is the default persistence mode cos it is simple and safe for small data sets. When you data set starts to increase considerably in size, think twice about snapshots. 

The second more durable persistence mode is Append Only File which was introduced in Redis 1.1 to solve the drawbacks of snapshotting. In this mode, every redis command, is appended to a file as a log. Very much like WAL logs for conventional RDBMSs. This way, you can build the entire database  by replaying the entire file. If can be argued that sequential writes to file are significantly faster than random access but is is still significantly slower than writing in memory, and this goes against the essence of what redis is, which is an in-memory data store. If you are going to be writing each command to a file, why not use a datastore that is designed for that in the first place. If you use AOF it means redis is going to call fsync at a point in time which can be configured in 1 of 3 ways. `appendfsync always` will call fsync on each command. This is very very safe. But with this option you might as well through redis, out the window, because the performance becomes infirior to every database designed to fsync on write. if you use this mode without a [Battery Backed Write Cache (BBWC) RAID controller](https://serverfault.com/questions/65096/battery-backed-write-cache), you will get fucked. Have fun figuring out what went wrong. `appendfsync everysec` will call `fsync` every second which means at most you loose a second of data. This might sound reasonable but is not without its drawbacks. if you have a update intensive application like a counter that updates many times in an second, you end up with a needlessly huge AOF file when in essence the data payload might only have a small foot print. that said is is an easier choice and has been the new default since Redis 2.4. `appendfsync no` delegates the calling of `fsync` to the operating system. This is the fastest and the least safe method amongst all the `appendfsync` options. Normally Linux will flush data every 30 seconds with this configuration, but it's up to the kernel's exact tuning.

Choosing a database is a decision with deep business implications. One you often cannot change easily. Choosing redis as a persistence solution with complete disregard for how it handles permanence never ends well. Knowing where redis shines is key to being abel to maximize it. If a failing redis results in the permanet loss of your data, you should probably think about how you can get of redis. Redis will fail on you. That doesnt prevent it from being awesome. 
